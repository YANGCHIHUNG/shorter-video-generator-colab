"""
æ··åˆå­—å¹•ç”Ÿæˆå™¨ - ä½¿ç”¨Whisperæ™‚é–“æˆ³ + ç”¨æˆ¶ç·¨è¼¯æ–‡å­—
Hybrid Subtitle Generator - Uses Whisper timestamps + User edited text
"""

import os
import re
import logging
import tempfile
import subprocess
from typing import List, Dict, Tuple, Optional
import whisper
from moviepy.editor import VideoFileClip

logger = logging.getLogger(__name__)

class HybridSubtitleGenerator:
    """
    æ··åˆå­—å¹•ç”Ÿæˆå™¨
    - ä½¿ç”¨ Whisper åˆ†æéŸ³é »ä¸¦ç²å–æ™‚é–“æˆ³
    - å°‡ç”¨æˆ¶ç·¨è¼¯çš„æ–‡å­—æŒ‰æ¯”ä¾‹åˆ†é…åˆ°æ™‚é–“æ®µ
    - ç”Ÿæˆæº–ç¢ºçš„ SRT å­—å¹•æª”æ¡ˆ
    """
    
    def __init__(self, model_size: str = "small", traditional_chinese: bool = False):
        """
        åˆå§‹åŒ–æ··åˆå­—å¹•ç”Ÿæˆå™¨
        
        Args:
            model_size: Whisperæ¨¡å‹å¤§å° ('tiny', 'small', 'medium', 'large')
            traditional_chinese: æ˜¯å¦è½‰æ›ç‚ºç¹é«”ä¸­æ–‡
        """
        self.model_size = model_size
        self.traditional_chinese = traditional_chinese
        self.model = None
        
        logger.info(f"ğŸ”§ HybridSubtitleGenerator initialized with model: {model_size}")
        
        # åˆå§‹åŒ–ç¹é«”ä¸­æ–‡è½‰æ›å™¨ï¼ˆå¦‚æœéœ€è¦ï¼‰
        if traditional_chinese:
            try:
                import opencc
                self.converter = opencc.OpenCC('s2t')
                self.use_opencc = True
                logger.info("âœ… Traditional Chinese conversion enabled (OpenCC)")
            except ImportError:
                try:
                    import zhconv
                    self.zhconv = zhconv
                    self.use_opencc = False
                    logger.info("âœ… Traditional Chinese conversion enabled (zhconv)")
                except ImportError:
                    self.use_opencc = None
                    logger.warning("âš ï¸ No Chinese conversion library available")
        else:
            self.use_opencc = None
    
    def _load_whisper_model(self):
        """å»¶é²è¼‰å…¥ Whisper æ¨¡å‹"""
        if self.model is None:
            logger.info(f"ğŸ“¥ Loading Whisper model: {self.model_size}")
            self.model = whisper.load_model(self.model_size)
            logger.info("âœ… Whisper model loaded successfully")
    
    def generate_hybrid_subtitles(self, video_path: str, reference_texts: List[str], 
                                output_srt_path: str = None) -> str:
        """
        ç”Ÿæˆæ··åˆå­—å¹•
        
        Args:
            video_path: è¦–é »æª”æ¡ˆè·¯å¾‘
            reference_texts: ç”¨æˆ¶ç·¨è¼¯çš„æ–‡å­—åˆ—è¡¨ï¼ˆæ¯å€‹å…ƒç´ å°æ‡‰ä¸€å€‹é é¢/æ®µè½ï¼‰
            output_srt_path: è¼¸å‡ºSRTæª”æ¡ˆè·¯å¾‘
        
        Returns:
            ç”Ÿæˆçš„SRTæª”æ¡ˆè·¯å¾‘
        """
        try:
            logger.info(f"ğŸ¬ Generating hybrid subtitles for: {video_path}")
            logger.info(f"ğŸ“ Reference texts: {len(reference_texts)} segments")
            
            # 1. å¾è¦–é »æå–éŸ³é »
            audio_path = self._extract_audio_from_video(video_path)
            
            # 2. ä½¿ç”¨ Whisper ç²å–æ™‚é–“æˆ³
            whisper_segments = self._get_whisper_timestamps(audio_path)
            
            # 3. å°‡ç”¨æˆ¶æ–‡å­—åˆ†é…åˆ°æ™‚é–“æˆ³
            hybrid_segments = self._map_text_to_timestamps(whisper_segments, reference_texts)
            
            # 4. ç”Ÿæˆ SRT æª”æ¡ˆ
            if output_srt_path is None:
                output_srt_path = video_path.replace('.mp4', '_hybrid.srt')
            
            self._write_srt_file(hybrid_segments, output_srt_path)
            
            # 5. æ¸…ç†æš«å­˜æª”æ¡ˆ
            try:
                os.unlink(audio_path)
            except Exception as e:
                logger.warning(f"âš ï¸ Failed to remove temp audio: {e}")
            
            logger.info(f"âœ… Hybrid subtitles generated: {output_srt_path}")
            return output_srt_path
            
        except Exception as e:
            logger.error(f"âŒ Error generating hybrid subtitles: {e}")
            raise
    
    def _extract_audio_from_video(self, video_path: str) -> str:
        """å¾è¦–é »æå–éŸ³é »"""
        try:
            with tempfile.NamedTemporaryFile(suffix='.wav', delete=False) as temp_audio:
                temp_audio_path = temp_audio.name
            
            # ä½¿ç”¨FFmpegæå–éŸ³é »
            cmd = [
                'ffmpeg', '-i', video_path,
                '-vn', '-acodec', 'pcm_s16le',
                '-ar', '16000', '-ac', '1',
                '-y', temp_audio_path
            ]
            
            result = subprocess.run(cmd, capture_output=True, text=True)
            if result.returncode != 0:
                logger.error(f"âŒ FFmpeg failed: {result.stderr}")
                raise Exception(f"Audio extraction failed: {result.stderr}")
            
            logger.info(f"ğŸµ Audio extracted to: {temp_audio_path}")
            return temp_audio_path
            
        except Exception as e:
            logger.error(f"âŒ Error extracting audio: {e}")
            raise
    
    def _get_whisper_timestamps(self, audio_path: str) -> List[Dict]:
        """ä½¿ç”¨ Whisper ç²å–æ™‚é–“æˆ³"""
        try:
            self._load_whisper_model()
            
            logger.info("ğŸ¤– Running Whisper transcription for timestamps...")
            
            # Whisper è½‰éŒ„é¸é …
            options = {
                "word_timestamps": True,
                "verbose": False,
                "language": "zh"  # æŒ‡å®šä¸­æ–‡ä»¥æé«˜æº–ç¢ºæ€§
            }
            
            result = self.model.transcribe(audio_path, **options)
            
            segments = result.get("segments", [])
            logger.info(f"â±ï¸ Whisper found {len(segments)} time segments")
            
            # è¨˜éŒ„æ™‚é–“æˆ³ä¿¡æ¯
            for i, segment in enumerate(segments[:3]):  # åªè¨˜éŒ„å‰3å€‹ä½œç‚ºç¤ºä¾‹
                logger.debug(f"   Segment {i+1}: {segment['start']:.2f}s - {segment['end']:.2f}s")
            
            return segments
            
        except Exception as e:
            logger.error(f"âŒ Error getting Whisper timestamps: {e}")
            raise
    
    def _map_text_to_timestamps(self, whisper_segments: List[Dict], 
                               reference_texts: List[str]) -> List[Dict]:
        """å°‡ç”¨æˆ¶æ–‡å­—åˆ†é…åˆ° Whisper æ™‚é–“æˆ³"""
        try:
            logger.info(f"ğŸ”„ Mapping {len(reference_texts)} texts to {len(whisper_segments)} timestamps")
            
            # å°‡æ‰€æœ‰åƒè€ƒæ–‡å­—åˆä½µä¸¦åˆ†å¥
            all_text = " ".join(reference_texts)
            sentences = self._split_text_into_sentences(all_text)
            
            logger.info(f"ğŸ“ Split into {len(sentences)} sentences")
            
            # å¦‚æœå¥å­æ•¸é‡èˆ‡æ™‚é–“æ®µæ•¸é‡ç›¸è¿‘ï¼Œç›´æ¥ä¸€ä¸€å°æ‡‰
            if abs(len(sentences) - len(whisper_segments)) <= 2:
                return self._direct_mapping(whisper_segments, sentences)
            
            # å¦å‰‡ï¼ŒæŒ‰æ™‚é–“æ¯”ä¾‹åˆ†é…
            return self._proportional_mapping(whisper_segments, sentences)
            
        except Exception as e:
            logger.error(f"âŒ Error mapping text to timestamps: {e}")
            raise
    
    def _split_text_into_sentences(self, text: str) -> List[str]:
        """å°‡æ–‡å­—åˆ†å‰²ç‚ºå¥å­"""
        # ä¸­æ–‡å¥å­åˆ†éš”ç¬¦
        sentences = re.split(r'[ã€‚ï¼ï¼Ÿ\.\!\?ï¼›;]', text)
        # ç§»é™¤ç©ºå­—ç¬¦ä¸²ä¸¦æ¸…ç†
        sentences = [s.strip() for s in sentences if s.strip()]
        return sentences
    
    def _direct_mapping(self, whisper_segments: List[Dict], 
                       sentences: List[str]) -> List[Dict]:
        """ç›´æ¥ä¸€ä¸€å°æ‡‰æ˜ å°„"""
        logger.info("ğŸ“ Using direct mapping")
        
        hybrid_segments = []
        min_length = min(len(whisper_segments), len(sentences))
        
        for i in range(min_length):
            segment = whisper_segments[i].copy()
            segment['text'] = self._convert_to_traditional(sentences[i])
            segment['original_text'] = segment.get('text', '')
            hybrid_segments.append(segment)
        
        # å¦‚æœé‚„æœ‰å‰©é¤˜çš„å¥å­ï¼Œåˆ†é…åˆ°æœ€å¾Œä¸€å€‹æ™‚é–“æ®µ
        if len(sentences) > min_length:
            remaining_text = " ".join(sentences[min_length:])
            if hybrid_segments:
                hybrid_segments[-1]['text'] += " " + self._convert_to_traditional(remaining_text)
        
        return hybrid_segments
    
    def _proportional_mapping(self, whisper_segments: List[Dict], 
                            sentences: List[str]) -> List[Dict]:
        """æŒ‰æ¯”ä¾‹åˆ†é…æ˜ å°„"""
        logger.info("ğŸ“Š Using proportional mapping")
        
        if not whisper_segments or not sentences:
            return []
        
        # è¨ˆç®—ç¸½æ™‚é•·
        total_duration = whisper_segments[-1]['end'] - whisper_segments[0]['start']
        
        # è¨ˆç®—æ¯å€‹å¥å­çš„ç›¸å°é•·åº¦ï¼ˆå­—ç¬¦æ•¸ï¼‰
        sentence_lengths = [len(s) for s in sentences]
        total_chars = sum(sentence_lengths)
        
        hybrid_segments = []
        current_time = whisper_segments[0]['start']
        
        for i, sentence in enumerate(sentences):
            # è¨ˆç®—é€™å€‹å¥å­æ‡‰è©²ä½”ç”¨çš„æ™‚é–“æ¯”ä¾‹
            char_ratio = sentence_lengths[i] / total_chars if total_chars > 0 else 1 / len(sentences)
            sentence_duration = total_duration * char_ratio
            
            # ç¢ºä¿æœ€å°æ™‚é•·
            sentence_duration = max(sentence_duration, 1.0)
            
            start_time = current_time
            end_time = min(current_time + sentence_duration, whisper_segments[-1]['end'])
            
            # å¦‚æœæ˜¯æœ€å¾Œä¸€å€‹å¥å­ï¼Œå»¶ä¼¸åˆ°æœ€å¾Œ
            if i == len(sentences) - 1:
                end_time = whisper_segments[-1]['end']
            
            hybrid_segments.append({
                'start': start_time,
                'end': end_time,
                'text': self._convert_to_traditional(sentence),
                'original_text': '',
                'mapped_method': 'proportional'
            })
            
            current_time = end_time
        
        return hybrid_segments
    
    def _convert_to_traditional(self, text: str) -> str:
        """è½‰æ›ç‚ºç¹é«”ä¸­æ–‡ï¼ˆå¦‚æœå•Ÿç”¨ï¼‰"""
        if not self.traditional_chinese or not text:
            return text
        
        try:
            if self.use_opencc is True and hasattr(self, 'converter'):
                return self.converter.convert(text)
            elif self.use_opencc is False and hasattr(self, 'zhconv'):
                return self.zhconv.convert(text, 'zh-tw')
            else:
                return text
        except Exception as e:
            logger.warning(f"âš ï¸ Traditional Chinese conversion failed: {e}")
            return text
    
    def _write_srt_file(self, segments: List[Dict], output_path: str):
        """å¯«å…¥ SRT æª”æ¡ˆ"""
        srt_content = ""
        
        for i, segment in enumerate(segments, 1):
            start_time = self._format_timestamp(segment['start'])
            end_time = self._format_timestamp(segment['end'])
            text = segment['text'].strip()
            
            if text:  # åªæ·»åŠ éç©ºç™½æ–‡å­—
                srt_content += f"{i}\n"
                srt_content += f"{start_time} --> {end_time}\n"
                srt_content += f"{text}\n\n"
        
        with open(output_path, 'w', encoding='utf-8') as f:
            f.write(srt_content)
        
        logger.info(f"ğŸ“„ SRT file written with {len(segments)} segments")
    
    def _format_timestamp(self, seconds: float) -> str:
        """å°‡ç§’æ•¸è½‰æ›ç‚º SRT æ™‚é–“æ ¼å¼"""
        hours = int(seconds // 3600)
        minutes = int((seconds % 3600) // 60)
        secs = int(seconds % 60)
        millisecs = int((seconds % 1) * 1000)
        
        return f"{hours:02d}:{minutes:02d}:{secs:02d},{millisecs:03d}"

    def embed_subtitles_in_video(self, input_video_path: str, srt_path: str,
                               output_video_path: str, subtitle_style: str = "default") -> bool:
        """å°‡å­—å¹•åµŒå…¥è¦–é »"""
        try:
            logger.info(f"ğŸ¬ Embedding subtitles into video...")
            
            # å­—å¹•æ¨£å¼è¨­å®š
            subtitle_styles = {
                "default": "FontSize=24,PrimaryColour=&Hffffff,OutlineColour=&H000000,Outline=2,Shadow=1",
                "yellow": "FontSize=24,PrimaryColour=&H00ffff,OutlineColour=&H000000,Outline=2,Shadow=1",
                "white_box": "FontSize=24,PrimaryColour=&Hffffff,BackColour=&H80000000,BorderStyle=4",
                "custom": "FontSize=26,PrimaryColour=&Hffffff,OutlineColour=&H000000,Outline=3,Shadow=2"
            }
            
            style = subtitle_styles.get(subtitle_style, subtitle_styles["default"])
            
            # ä½¿ç”¨ FFmpeg åµŒå…¥å­—å¹•
            cmd = [
                'ffmpeg',
                '-i', input_video_path,
                '-vf', f"subtitles={srt_path}:force_style='{style}'",
                '-c:a', 'copy',
                '-y', output_video_path
            ]
            
            result = subprocess.run(cmd, capture_output=True, text=True)
            
            if result.returncode == 0:
                logger.info(f"âœ… Subtitles embedded successfully")
                return True
            else:
                logger.error(f"âŒ FFmpeg failed: {result.stderr}")
                return False
                
        except Exception as e:
            logger.error(f"âŒ Error embedding subtitles: {e}")
            return False
